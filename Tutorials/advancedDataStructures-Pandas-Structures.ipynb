{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5af2b8f",
   "metadata": {},
   "source": [
    "# Pandas Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07cb97",
   "metadata": {},
   "source": [
    "Similar to NumPy, Pandas uses two data types: series (equivalent to vectors) and dataframes (equivalent to matrices). However, Pandas provides an additional layer of abstraction over the underlying array, including indices, column headers and database-style operations.\n",
    "\n",
    "Notice the indices and data type declaration in the following series output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6640a9",
   "metadata": {},
   "source": [
    "## Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7943f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.Series([0.25, 0.5, 0.75, 1.0])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c9434",
   "metadata": {},
   "source": [
    "A series (and a dataframe) can be decomposed to pull out its constituent elements. Series elements can be referenced similar to NumPy arrays. For dataframes, the translation from NumPy arrays is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e62cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5822b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf2215",
   "metadata": {},
   "source": [
    "Basic and advanced data structures can be drawn into Pandas objects. Indices need not be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "population['Texas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e798a0",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43fe16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's define a new series for states that gives their areas\n",
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "\n",
    "# We can combine population and area data series for states into a dataframe\n",
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ceff3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c461b15",
   "metadata": {},
   "source": [
    "Pandas automatically infers the data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d992ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.population.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d3ef9",
   "metadata": {},
   "source": [
    "There are many other ways to construct a dataframe. As we will see in subsequent tutorials, one common way is by reading in a csv file to a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70d07c",
   "metadata": {},
   "source": [
    "## Pandas indexing\n",
    "There are several ways to index data via Pandas. It can sometimes be confusing when indices are numerical but not sequential. Pandas can help address this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f440a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# explicit index when indexing - return the element with index value 1\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implicit index when slicing - return the elements at index locations 1 and 2\n",
    "data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fa24e",
   "metadata": {},
   "source": [
    "I find these results confusion. Fortunately, Pandas has a built-in wrapper that adds intuition for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c81576",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1:3] # Return the element with index values in the range 1 to 3 (inclusive). Can also provide a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d341ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1:3] # Return the element with index locations in the range 1 to 3 (exclusive of 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47dadb2",
   "metadata": {},
   "source": [
    "Dataframes can also reference columns using their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.Series({'California': 423967, 'Texas': 695662,\n",
    "                  'New York': 141297, 'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "pop = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
    "                 'New York': 19651127, 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec82c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a32bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[['area','pop']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb570ce",
   "metadata": {},
   "source": [
    "We can filter for both indices and column names with dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d295d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:'Florida',:'pop'] # Return all rows from 'Florida' and above and all columns from 'pop' and left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a5685",
   "metadata": {},
   "source": [
    "We can mix indices and column names using the ```.ix``` indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8daa4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.iloc[:3].loc[:,:'area'] # Return all rows with indices less than 3 (exclusive) and all colummns from 'area' and left (inclusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83251ec",
   "metadata": {},
   "source": [
    "## Operating on data in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40a506",
   "metadata": {},
   "source": [
    "Pandas is built on top of NumPy, so the vector optimization we saw with NumPy is also available with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec173cfc",
   "metadata": {},
   "source": [
    "### Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d0e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42) # uses the Mersenne Twister pseudo-random number generator\n",
    "ser = pd.Series(rng.randint(0, 10, 4))\n",
    "\n",
    "df = pd.DataFrame(rng.randint(0, 10, (3, 4)),\n",
    "                  columns=['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa560c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1315264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sin(df * np.pi / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e27ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(df.A/df.B * np.pi / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe7dce",
   "metadata": {},
   "source": [
    "### Detecting missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6392a03",
   "metadata": {},
   "source": [
    "A common data processing operation is indicating missing data. We can either indicate missing values using 1) a masking approach (e.g., a separate vector with ```1/0``` indicators) or 2) a sentinel value (e.g., ```-999```). The first approach adds overhead storage and computation requirements. Implementation of the second approach in Pandas depends on the data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae340288",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.array([1, None, 3, 4]) # Using None is the Pythonic approach. It requires datatype object.\n",
    "print(vals1.dtype)\n",
    "\n",
    "vals2 = np.array([1, np.nan, 3, 4]) # Using None is the Pythonic approach. It requires datatype object.\n",
    "print(vals2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29b8b4",
   "metadata": {},
   "source": [
    "The datatype will affect the treatment of missing values when performing arithmetic operations on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1.sum() # produces a TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4220b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals2.sum() # No error but gives nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19012ed",
   "metadata": {},
   "source": [
    "Pandas has two useful methods to detect null data: ```isnull()``` and ```notnull()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 'hello', None])\n",
    "data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbbbd1",
   "metadata": {},
   "source": [
    "It can be useful to combine these methods with masking to filter out NA values from a Pandas dataframe or series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc927835",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96dc3f",
   "metadata": {},
   "source": [
    "### Dropping missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebe670",
   "metadata": {},
   "source": [
    "Pandas also contains two convenience functions to deal with NA values: ```dropna()``` removes NA values and ```fillna()``` fills NA values with a user-specified value. Dataframes have the additional functionality that we can specify whether to drop rows (default implementation) or columns containing NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74801c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,      np.nan, 2],\n",
    "                   [2,      3,      5],\n",
    "                   [np.nan, 4,      6]])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16914b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad23a6",
   "metadata": {},
   "source": [
    "There is an even finer specification available. Perhaps we are interested in keeping only rows (columns) with 3 or more non-NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3dfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3] = np.nan # add some more NA to our dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48361a36",
   "metadata": {},
   "source": [
    "### Filling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3701f3",
   "metadata": {},
   "source": [
    "Fill NA values with a single value, say 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bef3a1",
   "metadata": {},
   "source": [
    "Fill NA values with the value from above in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75d585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forward-fill\n",
    "data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af099d33",
   "metadata": {},
   "source": [
    "Fill NA values with the value from below in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18733436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# back-fill\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e25d63",
   "metadata": {},
   "source": [
    "The same procedure as for Series, but we must specify an axis (default rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='bfill', axis=1) # backfill doesn't fill last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2182fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', axis=1) # forward doesn't fill last row of first column because no non-NA value before it in row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404bb65",
   "metadata": {},
   "source": [
    "## Hierarchical (multi-index) data\n",
    "Up to this point we've been focused primarily on one-dimensional and two-dimensional data, stored in Pandas ```Series``` and ```DataFrame``` objects, respectively. Often it is useful to go beyond this and store higher-dimensional data–that is, data indexed by more than one or two keys. It is common to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional ```Series``` and two-dimensional ```DataFrame``` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23945b",
   "metadata": {},
   "source": [
    "Suppose we would like to track data about states from two different years. Using the Pandas tools we've already covered, you might be tempted to simply use Python tuples as keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [('California', 2000), ('California', 2010),\n",
    "         ('New York', 2000), ('New York', 2010),\n",
    "         ('Texas', 2000), ('Texas', 2010)]\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "pop = pd.Series(populations, index=index)\n",
    "pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8d825",
   "metadata": {},
   "source": [
    "With this indexing scheme, we can straightforwardly index or slice the series based on this multiple index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eabdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop[('California', 2010):('Texas', 2000)] # note: this assumes we know the order that data appears in the series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e184ac",
   "metadata": {},
   "source": [
    "Consider the case that we are interested in all data from 2010. How would be select these data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f884cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop[[i for i in pop.index if i[1] == 2010]] # messy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb00b6",
   "metadata": {},
   "source": [
    "Pandas provides a much clearer built-in method for handling multi-index data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_tuples(index) # create a MultiIndex from the index tuple\n",
    "pop = pop.reindex(index) # change the pop index to our new MultiIndex\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop[:, 2010] # Much simpler than the non-MultiIndex approach above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a4680",
   "metadata": {},
   "source": [
    "Pandas also recognizes that a ```MultiIndex``` could be used to create a ```DataFrame``` from a ```Series```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pop.unstack()\n",
    "pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.stack() # opposite operation to unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f86304",
   "metadata": {},
   "source": [
    "```MultiIndex``` syntax becomes even more powerful when we move from a ```Series``` to a ```DataFrame```, which cannot be represented by creating a new dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pd.DataFrame({'total': pop,\n",
    "                       'under18': [9267089, 9284094,\n",
    "                                   4687374, 4318033,\n",
    "                                   5906301, 6879014]})\n",
    "pop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684af8f",
   "metadata": {},
   "source": [
    "We can combine the above functionalities with vector arithmetic to calculate the proportion of the population aged less than 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dae97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_u18 = pop_df['under18'] / pop_df['total']\n",
    "f_u18.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d2d9b",
   "metadata": {},
   "source": [
    "In a ```DataFrame```, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical indices and columns\n",
    "index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],\n",
    "                                   names=['year', 'visit']) # uses cartesian product (all combinations of levels) to generate the MultiIndex\n",
    "columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']],\n",
    "                                     names=['subject', 'type']) # uses cartesian product (all combinations of levels) to generate the MultiIndex\n",
    "\n",
    "# mock some data\n",
    "data = np.round(np.random.randn(4, 6), 1)\n",
    "data[:, ::2] *= 10\n",
    "data += 37\n",
    "\n",
    "# create the DataFrame\n",
    "health_data = pd.DataFrame(data, index=index, columns=columns)\n",
    "health_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfe163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "health_data['Guido', 'HR'] # access heartrate data for Guido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a65e84",
   "metadata": {},
   "source": [
    "We've previously seen that Pandas has built-in data aggregation methods, such as ```mean()```, ```sum()```, and ```max()```. For hierarchically indexed data, these can be passed a ```level``` parameter that controls which subset of the data the aggregate is computed on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034aa53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = health_data.groupby(level='year').mean() # groupby() syntax can also by used with columns by passing the string name (without level=)\n",
    "data_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de0913",
   "metadata": {},
   "source": [
    "## Combining datasets\n",
    "One of the most powerful functionalities provided by Pandas is the ability to combine ```DataFrames``` and ```Series``` based on common column or index values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066fa64",
   "metadata": {},
   "source": [
    "For convenience, we'll define a function to generate ```DataFrames```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a651508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(cols, ind):\n",
    "    \"\"\"Quickly make a DataFrame\"\"\"\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "            for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "\n",
    "# example DataFrame\n",
    "make_df('ABC', range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9deb522",
   "metadata": {},
   "source": [
    "In addition, we'll create a quick class that allows us to display multiple ```DataFrames``` side by side. The code makes use of the special ```_repr_html_``` method, which IPython (Jupyter) uses to implement its rich object display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c70f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9a1b2",
   "metadata": {},
   "source": [
    "### Combining data using ```concatenate()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18214f",
   "metadata": {},
   "source": [
    "The simplest approach is to use the ```concatenate()``` function, which works similar to the parallel function in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c77e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = make_df('AB', [1, 2])\n",
    "df2 = make_df('AB', [3, 4])\n",
    "display('df1', 'df2', 'pd.concat([df1, df2])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e937d3",
   "metadata": {},
   "source": [
    "Caution is warranted when using ```concatenate()``` because it can produce duplicate indices. Fortunately, Pandas has a built-in solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9caf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = make_df('AB', [0, 1])\n",
    "y = make_df('AB', [2, 3])\n",
    "y.index = x.index  # make duplicate indices!\n",
    "display('x', 'y', 'pd.concat([x, y])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # try to run a code snippet\n",
    "    pd.concat([x, y], verify_integrity=True)\n",
    "except ValueError as e: # if a value error is thrown, then run the below code\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b0516",
   "metadata": {},
   "source": [
    "It is often the case that the ```DataFrames``` to be joined do not have common column names. In such cases, we must use the ```join``` option.\n",
    "\n",
    "If not provided, ```concatenate()``` will default to using an outer join and fill missing values with ```NaN```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = make_df('ABC', [1, 2])\n",
    "df6 = make_df('BCD', [3, 4])\n",
    "display('df5', 'df6', 'pd.concat([df5, df6])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48183099",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df5', 'df6',\n",
    "        \"pd.concat([df5, df6], join='inner')\") # Only include columns that are common between the DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87452d",
   "metadata": {},
   "source": [
    "### Combining data using ```merge()```\n",
    "One essential feature offered by Pandas is its high-performance, in-memory join and merge operations. The main interface for this operation is the ```pd.merge``` function, and we'll see a few examples of how this can work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6004c3",
   "metadata": {},
   "source": [
    "Perhaps the simplest type of merge expresion is the one-to-one join, which is in many ways very similar to column-wise ```concatenation()```. In the following example, ```pd.merge()``` recognizes that both ```DataFrames``` contain an 'employee' column. It will merge these ```DataFrames``` using this common column as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "display('df1', 'df2', 'pd.merge(df1, df2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a0d1f",
   "metadata": {},
   "source": [
    "Often a ```DataFrame``` contains duplicate entries. This produces many-to-one joins. In the below example, there are multiple entries in ```df3``` for 'group' that match to 'supervisor' in ```df4```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2)\n",
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "display('df3', 'df4', 'pd.merge(df3, df4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e9574",
   "metadata": {},
   "source": [
    "We can also have a many-to-many relationship. This will create duplicate rows in the resulting ```DataFrame```, one for each row with common row values between ```df1``` and ```df5```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ece73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "display('df1', 'df5', \"pd.merge(df1, df5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ad969",
   "metadata": {},
   "source": [
    "In the preceding example, ```merge()``` is using the default behavior of matching along all common columns between the ```DataFrames```. Often, we want to merge ```DataFrames``` based on a subset of columns/indices, which may not have matching names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b519ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df1', 'df2', \"pd.merge(df1, df2, on='employee')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "display('df1', 'df6', 'pd.merge(df1, df6, left_on=\"employee\", right_on=\"name\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d38ff4",
   "metadata": {},
   "source": [
    "The above operation results in a duplicate column. We can drop this column using the ```drop()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8b5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df6, left_on=\"employee\", right_on=\"name\").drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cb282",
   "metadata": {},
   "source": [
    "Sometimes, it is useful to merge ```DataFrames``` on an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1251c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1a = df1.set_index('employee')\n",
    "df2a = df2.set_index('employee')\n",
    "display('df1a', 'df2a',\n",
    "        \"pd.merge(df1a, df2a, left_index=True, right_index=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf0daa",
   "metadata": {},
   "source": [
    "### Specifying set arithmetic for joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32dfb8a",
   "metadata": {},
   "source": [
    "In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a value appears in one key column but not the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ac4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n",
    "                    'food': ['fish', 'beans', 'bread']},\n",
    "                   columns=['name', 'food'])\n",
    "df8 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n",
    "                    'drink': ['wine', 'beer']},\n",
    "                   columns=['name', 'drink'])\n",
    "display('df7', 'df8', 'pd.merge(df7, df8)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae671ad",
   "metadata": {},
   "source": [
    "Here we have merged two datasets that have only a single 'name' entry in common: Mary. By default, the result contains the intersection of the two sets of inputs; this is what is known as an inner join. We can specify this explicitly using the how keyword, which defaults to ```\"inner\"```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066bf152",
   "metadata": {},
   "source": [
    "Other options for the how keyword are ```'outer'```, ```'left'```, and ```'right'```. An outer join returns a join over the union of the input columns, and fills in all missing values with NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='outer')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5917f9",
   "metadata": {},
   "source": [
    "The left join and right join return joins over the left entries and right entries, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b19c0",
   "metadata": {},
   "source": [
    "## Working with Time Series Data\n",
    "Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time-indexed data. Date and time data comes in a few flavors, which we will discuss here:\n",
    "- *Time stamps* reference particular moments in time (e.g., July 4th, 2015 at 7:00am).\n",
    "- *Time intervals* and *periods* reference a length of time between a particular beginning and end point; for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods comprising days).\n",
    "- *Time deltas* or *durations* reference an exact length of time (e.g., a duration of 22.56 seconds).\n",
    "\n",
    "In this section, we will introduce how to work with each of these types of date/time data in Pandas. This short section is by no means a complete guide to the time series tools available in Python or Pandas, but instead is intended as a broad overview of how you as a user should approach working with time series. We will start with a brief discussion of tools for dealing with dates and times in Python, before moving more specifically to a discussion of the tools provided by Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09863348",
   "metadata": {},
   "source": [
    "### Dates and Times in Python\n",
    "The Python world has a number of available representations of dates, times, deltas, and timespans. While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb5258",
   "metadata": {},
   "source": [
    "#### Native Python dates and times: ```datetime``` and ```dateutil```\n",
    "Python's basic objects for working with dates and times reside in the built-in ```datetime``` module. Along with the third-party ```dateutil``` module, you can use it to quickly perform a host of useful functionalities on dates and times. For example, you can manually build a date using the ```datetime``` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime(year=2015, month=7, day=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b041d89",
   "metadata": {},
   "source": [
    "Or, using the ```dateutil``` module, you can parse dates from a variety of string formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "date = parser.parse(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d63ef5",
   "metadata": {},
   "source": [
    "Once you have a datetime object, you can do operations like printing the day of the week. We'll use one of the standard string format codes for printing dates (```\"%A\"```), which you can read about in the [strftime section](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) of Python's [datetime documentation](https://docs.python.org/3/library/datetime.html). Documentation of other useful date utilities can be found in dateutil's [online documentation](http://labix.org/python-dateutil). A related package to be aware of is ```pytz```, which contains tools for working with the most difficult piece of time series data: time zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0caedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b205a",
   "metadata": {},
   "source": [
    "The power of ```datetime``` and ```dateutil``` lie in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times: just as lists of Python numerical variables are suboptimal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aec199",
   "metadata": {},
   "source": [
    "#### Dates and times in Pandas\n",
    "Pandas builds upon all the tools just discussed to provide a ```Timestamp``` object, which combines the ease-of-use of ```datetime``` and ```dateutil`` with the efficient storage and vectorized interface of ```numpy.datetime64```. From a group of these ```Timestamp``` objects, Pandas can construct a ```DatetimeIndex``` that can be used to index data in a ```Series``` or ```DataFrame```.\n",
    "\n",
    "For example, we can use Pandas tools to repeat the demonstration from above. We can parse a flexibly formatted string date, and use format codes to output the day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d55b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc509289",
   "metadata": {},
   "outputs": [],
   "source": [
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f8199",
   "metadata": {},
   "source": [
    "Additionally, we can do NumPy-style vectorized operations directly on this same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "date + pd.to_timedelta(np.arange(12), 'D') # create a daily time series from the initial date to 12 days later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda999b",
   "metadata": {},
   "source": [
    "#### Pandas time series: Indexing by time\n",
    "Where the Pandas time series tools really become useful is when you begin to index data by timestamps. For example, we can construct a ```Series``` object that has time indexed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "                          '2015-07-04', '2015-08-04'])\n",
    "data = pd.Series([0, 1, 2, 3], index=index)\n",
    "data['2014-07-04':'2015-07-04'] # select data for specific dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9aa50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['2015'] # select data for specific year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c90d4",
   "metadata": {},
   "source": [
    "#### Pandas time series data structures\n",
    "This section will introduce the fundamental Pandas data structures for working with time series data:\n",
    "- For *time stamps*, Pandas provides the ```Timestamp``` type. As mentioned before, it is essentially a replacement for Python's native ```datetime```, but is based on the more efficient ```numpy.datetime64``` data type. The associated Index structure is ```DatetimeIndex```.\n",
    "- For *time Periods*, Pandas provides the ```Period``` type. This encodes a fixed-frequency interval based on ```numpy.datetime64```. The associated index structure is ```PeriodIndex```.\n",
    "- For *time deltas* or *durations*, Pandas provides the ```Timedelta``` type. ```Timedelta``` is a more efficient replacement for Python's native ```datetime.timedelta``` type, and is based on ```numpy.timedelta64```. The associated index structure is ```TimedeltaIndex```.\n",
    "\n",
    "The most fundamental of these date/time objects are the ```Timestamp``` and ```DatetimeIndex``` objects. While these class objects can be invoked directly, it is more common to use the ```pd.to_datetime()``` function, which can parse a wide variety of formats. Passing a single date to ```pd.to_datetime()``` yields a ```Timestamp```; passing a series of dates by default yields a ```DatetimeIndex```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c505d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',\n",
    "                       '2015-Jul-6', '07-07-2015', '20150708'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fa459",
   "metadata": {},
   "source": [
    "Any ```DatetimeIndex``` can be converted to a ```PeriodIndex``` with the ```to_period()``` function with the addition of a frequency code; here we'll use ```'D'``` to indicate daily frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11477673",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.to_period('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d984a4",
   "metadata": {},
   "source": [
    "A ```TimedeltaIndex``` is created, for example, when a date is subtracted from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382469e5",
   "metadata": {},
   "source": [
    "#### Regular sequences\n",
    "To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: ```pd.date_range()``` for timestamps, ```pd.period_range()``` for periods, and ```pd.timedelta_range()``` for time deltas. We've seen that Python's ```range()``` and NumPy's ```np.arange()``` turn a startpoint, endpoint, and optional stepsize into a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the frequency is one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', '2015-07-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296ccaa",
   "metadata": {},
   "source": [
    "Similar functionalities exist for time date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4492aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=10, freq='H') # generate a time sequence with 10 entries at an hourly frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6b4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
