{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5af2b8f",
   "metadata": {},
   "source": [
    "# Pandas Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df35e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f14115",
   "metadata": {},
   "source": [
    "## Aggregation and Grouping\n",
    "Here we will use the Planets dataset, available via the Seaborn package. It gives information on planets that astronomers have discovered around other stars (known as extrasolar planets or exoplanets for short). It can be downloaded with a simple Seaborn command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca93b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "planets = sns.load_dataset('planets')\n",
    "planets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39226872",
   "metadata": {},
   "source": [
    "Earlier, we explored some of the data aggregations available for NumPy ```arrays```. As with a one-dimensional NumPy ```array```, for a Pandas ```Series``` the aggregates return a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ser = pd.Series(rng.rand(5))\n",
    "print(\"Series sum:\", ser.sum())\n",
    "print(\"Series mean:\", ser.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c758972",
   "metadata": {},
   "source": [
    "For a ```DataFrame```, by default the aggregates return results within each column. By specifying the ```axis``` argument, you can instead aggregate within each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': rng.rand(5),\n",
    "                   'B': rng.rand(5)})\n",
    "print(\"Default aggregation by column for mean:\\n\", df.mean())\n",
    "print(\"Aggregation by row for mean:\\n\", df.mean(axis=\"columns\")) # could also use axis=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9ed49",
   "metadata": {},
   "source": [
    "In addition to standard aggregation functions, Pandas includes a convenience function called ```describe()``` that computes several common aggregates for each column and returns the result. Let's take a look at how it works using the planets dataset and dropping rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4515ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "planets.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58c022",
   "metadata": {},
   "source": [
    "This can be a useful way to begin understanding the overall properties of a dataset. For example, we see in the ```year``` column that although exoplanets were discovered as far back as 1989, half of all known expolanets were not discovered until 2010 or after. This is largely thanks to the Kepler mission, which is a space-based telescope specifically designed for finding eclipsing planets around other stars.\n",
    "\n",
    "To go deeper into the data, however, simple aggregates are often not enough. The next level of data summarization is the ```groupby``` operation, which allows you to quickly and efficiently compute aggregates on subsets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa602d54",
   "metadata": {},
   "source": [
    "## GroupBy: Split, Apply, Combine\n",
    "Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so-called groupby operation. The name \"group by\" comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by the ```R``` developer Hadley Wickham: split, apply, combine.\n",
    "\n",
    "![](figures/03.08-split-apply-combine.png)\n",
    "\n",
    "This makes clear what the ```groupby``` accomplishes:\n",
    "- The *split* step involves breaking up and grouping a ```DataFrame``` depending on the value of the specified key.\n",
    "- The *apply* step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups.\n",
    "- The *combine* step merges the results of these operations into an output array.\n",
    "\n",
    "While this could certainly be done manually using some combination of the masking, aggregation, and merging commands covered earlier, an important realization is that the intermediate splits do not need to be explicitly instantiated. Rather, the ```GroupBy``` can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way. The power of the ``GroupBy`` is that it abstracts away these steps: the user need not think about how the computation is done under the hood, but rather thinks about the operation as a whole.\n",
    "\n",
    "As a concrete example, let's take a look at using Pandas for the computation shown in this diagram using the planets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1db2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets.groupby('method')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c31390",
   "metadata": {},
   "source": [
    "Notice that what is returned is not a set of ```DataFrames```, but a ```DataFrameGroupBy``` object. This object is where the magic is: you can think of it as a special view of the ```DataFrame```, which does no actual computation until the aggregation is applied. This \"lazy evaluation\" approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user.The ```GroupBy``` object supports column indexing in the same way as a ```DataFrame```, and returns a modified GroupBy object. The ```GroupBy``` object supports column indexing in the same way as a ```DataFrame```, and returns a modified ```GroupBy``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61aee59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets.groupby('method')['orbital_period'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02c992",
   "metadata": {},
   "source": [
    "### Aggregate, fileter, transform, apply\n",
    "The preceding discussion focused on aggregation for the combine operation, but there are more options available. In particular, ```GroupBy``` objects have ```aggregate()```, ```filter()```, ```transform()```, and ```apply()``` methods that efficiently implement a variety of useful operations before combining the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0, 10, 6)},\n",
    "                   columns = ['key', 'data1', 'data2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca065b",
   "metadata": {},
   "source": [
    "#### Aggregation\n",
    "We're now familiar with ```GroupBy``` aggregations with ```sum()```, ```median()```, etc., but the aggregate() method allows for even more flexibility. It can take a string, a function, or a list thereof, and compute all the aggregates at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key').aggregate(['min', np.median, max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ece7b7",
   "metadata": {},
   "source": [
    "Another useful pattern is to pass a dictionary mapping column names to operations to be applied on that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662cc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key').aggregate({'data1': 'min',\n",
    "                             'data2': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030966a8",
   "metadata": {},
   "source": [
    "#### Filtering\n",
    "A filtering operation allows you to drop data based on the group properties. For example, we might want to keep all groups in which the standard deviation is larger than some critical value (e.g., 4 for column ```data2```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d603d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(x):\n",
    "    return x['data2'].std() > 4\n",
    "\n",
    "display('df', \"df.groupby('key').std()\", \"df.groupby('key').filter(filter_func)\") # Drops rows where key = A based on std value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbbbf1",
   "metadata": {},
   "source": [
    "#### Transformation\n",
    "While aggregation must return a reduced version of the data, transformation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea36793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key').transform(lambda x: x - x.mean()) # A lambda function is a Pythonic way to quickly specify a function in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5fd9f",
   "metadata": {},
   "source": [
    "#### Apply\n",
    "The ```apply()``` method lets you apply an arbitrary function to the group results. The function should take a ```DataFrame```, and return either a Pandas object (e.g., ```DataFrame```, ```Series```) or a scalar; the combine operation will be tailored to the type of output returned.\n",
    "\n",
    "For example, below is an ```apply()``` that normalizes the first column by the sum of the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18decb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def norm_by_data2(x):\n",
    "    # x is a DataFrame of group values\n",
    "    x['data1'] /= x['data2'].sum()\n",
    "    return x\n",
    "\n",
    "display('df', \"df.groupby('key').apply(norm_by_data2)\") # Groupby key and apply summation of data2 column sum for each group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f971e99",
   "metadata": {},
   "source": [
    "### Specifying the split key\n",
    "In the simple examples presented before, we split the ```DataFrame``` on a single column name. This is just one of many options by which the groups can be defined, and we'll go through some other options for group specification here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610efa2",
   "metadata": {},
   "source": [
    "##### A list, series, or index as the grouping keys\n",
    "The key can be any ```series```, ```list```, or ```index``` with a length matching that of the ```DataFrame```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [0, 1, 0, 1, 2, 0]\n",
    "display('df', 'df.groupby(L).sum()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507601c",
   "metadata": {},
   "source": [
    "#### A dictionary or series mapping index to group\n",
    "Another method is to provide a ```dictionary``` that maps index values to the group keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2b26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df.set_index('key')\n",
    "mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'}\n",
    "display('df2', 'df2.groupby(mapping).sum()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f870d",
   "metadata": {},
   "source": [
    "It is also possible to combine methods to form a multi-index grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c35b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.groupby(['key', mapping]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491f9aa",
   "metadata": {},
   "source": [
    "Below is a more complex example that combines several Python operations (some we've covered to this point and others that are noted in comments). This shows the power of combining many of the operations we've discussed up to this point when looking at realistic datasets. We immediately gain a coarse understanding of when and how planets have been discovered over the past several decades!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158dd90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decade = 10 * (planets['year'] // 10) # // is floor division that rounds down to the nearest integer\n",
    "decade = decade.astype(str) + 's' # Changes the integer decade into a string and adds 's' to the end of each decade: e.g., 1980s\n",
    "decade.name = 'decade' # Gives the Series a name 'decade'\n",
    "planets.groupby(['method', decade])['number'].sum().unstack().fillna(0) # Group planets DataFrame by method and decade Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c5e0a",
   "metadata": {},
   "source": [
    "## Pivot tables\n",
    "We have seen how the ```GroupBy``` abstraction lets us explore relationships within a dataset. A *pivot table* is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data. The *pivot table* takes simple column-wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summary of the data. A *pivot table* is essentially a multidimensional version of ```GroupBy``` aggregation. That is, you split-apply-combine, but both the split and the combine happen across a two-dimensional grid  rather than a one-dimensional index.\n",
    "\n",
    "For the examples in this section, we'll use the database of passengers on the Titanic available through the Seaborn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161fc4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1fcf66",
   "metadata": {},
   "source": [
    "### Manual pivot using Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da8345",
   "metadata": {},
   "source": [
    "It is possible to use ```Groupby``` to perform pivot table operations, but it can be cumbersome. To start learning more about this data, we might begin by grouping according to gender, survival status, or some combination thereof. Let's look at survival rate by gender using ```Groupby``` operations. This immediately gives us some insight: overall, three of every four females on board survived, while only one in five males survived!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('sex')[['survived']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b7cb9",
   "metadata": {},
   "source": [
    "This is useful, but we might like to go one step deeper and look at survival by both sex and, say, class. Using the vocabulary of ```GroupBy```, we might proceed using something like this: we group by class and gender, select survival, apply a mean aggregate, combine the resulting groups, and then unstack the hierarchical index to reveal the hidden multidimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33707bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1132d",
   "metadata": {},
   "source": [
    "### Pivot using built-in pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cfbcb",
   "metadata": {},
   "source": [
    "Two-dimensional ```GroupBy``` is common enough that Pandas includes a convenience routine, ```pivot_table```, which succinctly handles this type of multi-dimensional aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd782420",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index='sex', columns='class') # default aggfunc is mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index='sex', columns='class', aggfunc='std') # specify a different aggfunc 'std'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc93da",
   "metadata": {},
   "source": [
    "### Multi-level pivot tables\n",
    "Just as in the ```GroupBy```, the grouping in pivot tables can be specified with multiple levels, and via a number of options. For example, we might be interested in looking at age as a third dimension. We'll bin the age using the ```pd.cut``` function. Further, we can apply the same strategy when working with the columns as well; let's add info on the fare paid using pd.qcut to automatically compute quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.cut(titanic['age'], [0, 18, 80])\n",
    "fare = pd.qcut(titanic['fare'], 2)\n",
    "titanic.pivot_table('survived', index=['sex', age], columns=[fare, 'class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa301eb",
   "metadata": {},
   "source": [
    "### Additional pivot table options\n",
    "The full call signature of the ```pivot_table``` method of ```DataFrames``` is as follows:\n",
    "```\n",
    "# call signature as of Pandas 0.18\n",
    "DataFrame.pivot_table(data, values=None, index=None, columns=None,\n",
    "                      aggfunc='mean', fill_value=None, margins=False,\n",
    "                      dropna=True, margins_name='All')\n",
    "```\n",
    "Two of the options, ```fill_value``` and ```dropna```, have to do with missing data and are fairly straightforward; we will not show examples of them here.\n",
    "\n",
    "The ```aggfunc``` keyword controls what type of aggregation is applied, which is a mean by default. As in the GroupBy, the aggregation specification can be a string representing one of several common choices (e.g., ```'sum'```, ```'mean'```, ```'count'```, ```'min'```, ```'max'```, etc.) or a function that implements an aggregation (e.g., ```np.sum()```, ```min()```, ```sum()```, etc.). Additionally, it can be specified as a dictionary mapping a column to any of the above desired options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1449dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(index='sex', columns='class',\n",
    "                    aggfunc={'survived':'sum', 'fare':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7c97b",
   "metadata": {},
   "source": [
    "At times it's useful to compute totals along each grouping. This can be done via the margins keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index='sex', columns='class', margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e0236",
   "metadata": {},
   "source": [
    "## References\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
